{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PngIJSPB0EBy"
   },
   "source": [
    "#Reproducing results from paper : Deep Learning for ECG Classification\n",
    "#Link to Original Paper For Dataset Information : https://iopscience.iop.org/article/10.1088/1742-6596/913/1/012004\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LeQ08ZCARRR"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2epMuTKr1ISZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras import regularizers\n",
    "# from keras.utils import np_utils\n",
    "from keras import utils\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import signal\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dw9TJE8m0AGR"
   },
   "source": [
    "#Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "244eUgEBv1NS"
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y5-TAHpCwrSo"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y): # 0. >> [1. 0. 0. 0.]\n",
    "    return utils.to_categorical(y)\n",
    "\n",
    "\n",
    "def change(x):  #  [1. 0. 0. 0.]  >> 0 \n",
    "    answer = np.zeros((np.shape(x)[0]))\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_value = max(x[i, :])\n",
    "        max_index = list(x[i, :]).index(max_value)\n",
    "        answer[i] = max_index\n",
    "    return answer.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62sMhVKF1bXH"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl2KSMNg5eYY",
    "outputId": "17b8078c-7113-4d17-d1b0-37cca1906bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training size is  8528\n",
      "           0  1\n",
      "0     A00001  N\n",
      "1     A00002  N\n",
      "2     A00003  N\n",
      "3     A00004  A\n",
      "4     A00005  A\n",
      "...      ... ..\n",
      "8523  A08524  N\n",
      "8524  A08525  O\n",
      "8525  A08526  N\n",
      "8526  A08527  N\n",
      "8527  A08528  N\n",
      "\n",
      "[8528 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "number_of_classes = 4  # Total number of classes\n",
    "\n",
    "mypath = 'data/training2017/'\n",
    "onlyfiles = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f[0] == 'A')]\n",
    "\n",
    "# print(onlyfiles)\n",
    "\n",
    "bats = [f for f in onlyfiles if f[7] == 'm']\n",
    "\n",
    "# print(bats)\n",
    "\n",
    "check = 100\n",
    "\n",
    "mats = [f for f in bats if (np.shape(sio.loadmat(mypath + f)['val'])[1] >= check)]\n",
    "# print(mats)\n",
    "\n",
    "size = len(mats)\n",
    "print('Total training size is ', size)\n",
    "\n",
    "big = 10100\n",
    "X = np.zeros((size, big))\n",
    "\n",
    "for i in range(size):\n",
    "    dummy = sio.loadmat(mypath + mats[i])['val'][0, :]\n",
    "    if (big - len(dummy)) <= 0:\n",
    "        X[i, :] = dummy[0:big]\n",
    "    else:\n",
    "        b = dummy[0:(big - len(dummy))]\n",
    "        goal = np.hstack((dummy, b))\n",
    "        while len(goal) != big:\n",
    "            b = dummy[0:(big - len(goal))]\n",
    "            goal = np.hstack((goal, b))\n",
    "        X[i, :] = goal\n",
    "\n",
    "target_train = np.zeros((size, 1))\n",
    "\n",
    "\n",
    "Train_data = pd.read_csv(mypath + 'REFERENCE.csv', sep=',', header=None, names=None)\n",
    "print(Train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dhKpHTaqwui8"
   },
   "outputs": [],
   "source": [
    "for i in range(size):\n",
    "    if Train_data.loc[Train_data[0] == mats[i][:6], 1].values == 'N':\n",
    "        target_train[i] = 0\n",
    "    elif Train_data.loc[Train_data[0] == mats[i][:6], 1].values == 'A':\n",
    "        target_train[i] = 1\n",
    "    elif Train_data.loc[Train_data[0] == mats[i][:6], 1].values == 'O':\n",
    "        target_train[i] = 2\n",
    "    else:\n",
    "        target_train[i] = 3\n",
    "\n",
    "Label_set = to_one_hot(target_train)\n",
    "\n",
    "X = (X - X.mean()) / (X.std())  # Some normalization here\n",
    "X = np.expand_dims(X, axis=2)  # For Keras's data input size\n",
    "\n",
    "values = [i for i in range(size)]\n",
    "permutations = np.random.permutation(values)\n",
    "X = X[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "train = 0.9  # Size of training set in percentage\n",
    "X_train = X[:int(train * size), :]\n",
    "Y_train = Label_set[:int(train * size), :]\n",
    "X_val = X[int(train * size):, :]\n",
    "Y_val = Label_set[int(train * size):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving data\n",
    "# Save the array to a binary file\n",
    "np.save('savedData\\Xtrain.npy', X_train)\n",
    "np.save('savedData\\Ytrain.npy', Y_train)\n",
    "np.save('savedData\\Xval.npy', X_val)\n",
    "np.save('savedData\\Yval.npy', Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "X_train=np.load('savedData\\Xtrain.npy')\n",
    "Y_train=np.load('savedData\\Ytrain.npy')\n",
    "X_val =np.load('savedData\\Xval.npy')\n",
    "Y_val =np.load('savedData\\Yval.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4yi_VZeCwKz"
   },
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yghA565Xv1U9",
    "outputId": "5631d094-13ba-451d-be2d-e44da9ae617a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 10046, 128)        7168      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 1004, 128)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1004, 128)         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 980, 128)          409728    \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 196, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 196, 128)          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 187, 128)          163968    \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 37, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 37, 128)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 33, 128)           82048     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 737348 (2.81 MB)\n",
      "Trainable params: 737348 (2.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def create_model():\n",
    "model = Sequential()\n",
    "model.add(Conv1D(128, 55, activation='relu', input_shape=(big, 1)))\n",
    "model.add(MaxPooling1D(10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 25, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(128, 10, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_of_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkvCgWx2C4yL"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5g9KIR-C1BR",
    "outputId": "4cb74b74-9a67-403c-a79e-a2733af0d18e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 - 168s - loss: 1.1655 - accuracy: 0.5491 - val_loss: 1.0333 - val_accuracy: 0.5862 - 168s/epoch - 6s/step\n",
      "Epoch 2/60\n",
      "30/30 - 169s - loss: 1.0014 - accuracy: 0.5880 - val_loss: 0.9438 - val_accuracy: 0.5862 - 169s/epoch - 6s/step\n",
      "Epoch 3/60\n",
      "30/30 - 182s - loss: 0.9513 - accuracy: 0.5928 - val_loss: 0.9043 - val_accuracy: 0.5862 - 182s/epoch - 6s/step\n",
      "Epoch 4/60\n",
      "30/30 - 181s - loss: 0.8726 - accuracy: 0.5945 - val_loss: 0.8105 - val_accuracy: 0.6155 - 181s/epoch - 6s/step\n",
      "Epoch 5/60\n",
      "30/30 - 183s - loss: 0.8095 - accuracy: 0.6452 - val_loss: 0.8439 - val_accuracy: 0.6471 - 183s/epoch - 6s/step\n",
      "Epoch 6/60\n",
      "30/30 - 168s - loss: 0.7657 - accuracy: 0.6823 - val_loss: 0.7339 - val_accuracy: 0.6800 - 168s/epoch - 6s/step\n",
      "Epoch 7/60\n",
      "30/30 - 181s - loss: 0.7045 - accuracy: 0.7144 - val_loss: 0.6606 - val_accuracy: 0.7140 - 181s/epoch - 6s/step\n",
      "Epoch 8/60\n",
      "30/30 - 183s - loss: 0.6669 - accuracy: 0.7285 - val_loss: 0.6456 - val_accuracy: 0.7140 - 183s/epoch - 6s/step\n",
      "Epoch 9/60\n",
      "30/30 - 182s - loss: 0.6443 - accuracy: 0.7382 - val_loss: 0.5981 - val_accuracy: 0.7468 - 182s/epoch - 6s/step\n",
      "Epoch 10/60\n",
      "30/30 - 180s - loss: 0.6040 - accuracy: 0.7506 - val_loss: 0.5528 - val_accuracy: 0.7796 - 180s/epoch - 6s/step\n",
      "Epoch 11/60\n",
      "30/30 - 165s - loss: 0.5832 - accuracy: 0.7640 - val_loss: 0.5365 - val_accuracy: 0.8007 - 165s/epoch - 6s/step\n",
      "Epoch 12/60\n",
      "30/30 - 179s - loss: 0.5648 - accuracy: 0.7726 - val_loss: 0.5437 - val_accuracy: 0.8077 - 179s/epoch - 6s/step\n",
      "Epoch 13/60\n",
      "30/30 - 177s - loss: 0.5612 - accuracy: 0.7853 - val_loss: 0.5260 - val_accuracy: 0.7784 - 177s/epoch - 6s/step\n",
      "Epoch 14/60\n",
      "30/30 - 180s - loss: 0.5302 - accuracy: 0.7905 - val_loss: 0.5061 - val_accuracy: 0.8054 - 180s/epoch - 6s/step\n",
      "Epoch 15/60\n",
      "30/30 - 180s - loss: 0.5208 - accuracy: 0.7922 - val_loss: 0.4911 - val_accuracy: 0.8054 - 180s/epoch - 6s/step\n",
      "Epoch 16/60\n",
      "30/30 - 182s - loss: 0.5057 - accuracy: 0.8060 - val_loss: 0.4880 - val_accuracy: 0.8101 - 182s/epoch - 6s/step\n",
      "Epoch 17/60\n",
      "30/30 - 180s - loss: 0.4924 - accuracy: 0.8156 - val_loss: 0.4725 - val_accuracy: 0.8265 - 180s/epoch - 6s/step\n",
      "Epoch 18/60\n",
      "30/30 - 172s - loss: 0.5079 - accuracy: 0.8113 - val_loss: 0.4835 - val_accuracy: 0.8288 - 172s/epoch - 6s/step\n",
      "Epoch 19/60\n",
      "30/30 - 174s - loss: 0.4789 - accuracy: 0.8249 - val_loss: 0.4640 - val_accuracy: 0.8429 - 174s/epoch - 6s/step\n",
      "Epoch 20/60\n",
      "30/30 - 180s - loss: 0.4819 - accuracy: 0.8225 - val_loss: 0.4734 - val_accuracy: 0.8335 - 180s/epoch - 6s/step\n",
      "Epoch 21/60\n",
      "30/30 - 180s - loss: 0.4693 - accuracy: 0.8262 - val_loss: 0.4900 - val_accuracy: 0.8206 - 180s/epoch - 6s/step\n",
      "Epoch 22/60\n",
      "30/30 - 166s - loss: 0.4630 - accuracy: 0.8298 - val_loss: 0.4726 - val_accuracy: 0.8370 - 166s/epoch - 6s/step\n",
      "Epoch 23/60\n",
      "30/30 - 164s - loss: 0.4498 - accuracy: 0.8354 - val_loss: 0.4507 - val_accuracy: 0.8417 - 164s/epoch - 5s/step\n",
      "Epoch 24/60\n",
      "30/30 - 165s - loss: 0.4532 - accuracy: 0.8340 - val_loss: 0.4589 - val_accuracy: 0.8523 - 165s/epoch - 6s/step\n",
      "Epoch 25/60\n",
      "30/30 - 169s - loss: 0.4439 - accuracy: 0.8399 - val_loss: 0.4604 - val_accuracy: 0.8441 - 169s/epoch - 6s/step\n",
      "Epoch 26/60\n",
      "30/30 - 167s - loss: 0.4331 - accuracy: 0.8410 - val_loss: 0.4518 - val_accuracy: 0.8441 - 167s/epoch - 6s/step\n",
      "Epoch 27/60\n",
      "30/30 - 166s - loss: 0.4344 - accuracy: 0.8447 - val_loss: 0.4652 - val_accuracy: 0.8476 - 166s/epoch - 6s/step\n",
      "Epoch 28/60\n",
      "30/30 - 165s - loss: 0.4410 - accuracy: 0.8410 - val_loss: 0.4329 - val_accuracy: 0.8605 - 165s/epoch - 5s/step\n",
      "Epoch 29/60\n",
      "30/30 - 166s - loss: 0.4220 - accuracy: 0.8450 - val_loss: 0.4593 - val_accuracy: 0.8429 - 166s/epoch - 6s/step\n",
      "Epoch 30/60\n",
      "30/30 - 165s - loss: 0.4168 - accuracy: 0.8507 - val_loss: 0.4432 - val_accuracy: 0.8511 - 165s/epoch - 6s/step\n",
      "Epoch 31/60\n",
      "30/30 - 165s - loss: 0.4202 - accuracy: 0.8446 - val_loss: 0.4368 - val_accuracy: 0.8476 - 165s/epoch - 5s/step\n",
      "Epoch 32/60\n",
      "30/30 - 165s - loss: 0.4099 - accuracy: 0.8509 - val_loss: 0.4494 - val_accuracy: 0.8429 - 165s/epoch - 5s/step\n",
      "Epoch 33/60\n",
      "30/30 - 164s - loss: 0.3962 - accuracy: 0.8582 - val_loss: 0.4450 - val_accuracy: 0.8511 - 164s/epoch - 5s/step\n",
      "Epoch 34/60\n",
      "30/30 - 164s - loss: 0.4013 - accuracy: 0.8536 - val_loss: 0.4407 - val_accuracy: 0.8499 - 164s/epoch - 5s/step\n",
      "Epoch 35/60\n",
      "30/30 - 169s - loss: 0.3945 - accuracy: 0.8541 - val_loss: 0.4847 - val_accuracy: 0.8441 - 169s/epoch - 6s/step\n",
      "Epoch 36/60\n",
      "30/30 - 179s - loss: 0.3921 - accuracy: 0.8586 - val_loss: 0.4492 - val_accuracy: 0.8453 - 179s/epoch - 6s/step\n",
      "Epoch 37/60\n",
      "30/30 - 174s - loss: 0.3966 - accuracy: 0.8512 - val_loss: 0.5048 - val_accuracy: 0.8335 - 174s/epoch - 6s/step\n",
      "Epoch 38/60\n",
      "30/30 - 169s - loss: 0.3853 - accuracy: 0.8605 - val_loss: 0.4258 - val_accuracy: 0.8523 - 169s/epoch - 6s/step\n",
      "Epoch 39/60\n",
      "30/30 - 174s - loss: 0.3874 - accuracy: 0.8605 - val_loss: 0.4445 - val_accuracy: 0.8535 - 174s/epoch - 6s/step\n",
      "Epoch 40/60\n",
      "30/30 - 178s - loss: 0.3756 - accuracy: 0.8648 - val_loss: 0.4399 - val_accuracy: 0.8312 - 178s/epoch - 6s/step\n",
      "Epoch 41/60\n",
      "30/30 - 178s - loss: 0.3737 - accuracy: 0.8619 - val_loss: 0.4221 - val_accuracy: 0.8605 - 178s/epoch - 6s/step\n",
      "Epoch 42/60\n",
      "30/30 - 177s - loss: 0.3665 - accuracy: 0.8671 - val_loss: 0.4419 - val_accuracy: 0.8523 - 177s/epoch - 6s/step\n",
      "Epoch 43/60\n",
      "30/30 - 181s - loss: 0.3761 - accuracy: 0.8624 - val_loss: 0.4387 - val_accuracy: 0.8558 - 181s/epoch - 6s/step\n",
      "Epoch 44/60\n",
      "30/30 - 178s - loss: 0.3749 - accuracy: 0.8616 - val_loss: 0.4488 - val_accuracy: 0.8546 - 178s/epoch - 6s/step\n",
      "Epoch 45/60\n",
      "30/30 - 181s - loss: 0.3596 - accuracy: 0.8705 - val_loss: 0.4412 - val_accuracy: 0.8406 - 181s/epoch - 6s/step\n",
      "Epoch 46/60\n",
      "30/30 - 179s - loss: 0.3743 - accuracy: 0.8641 - val_loss: 0.4404 - val_accuracy: 0.8581 - 179s/epoch - 6s/step\n",
      "Epoch 47/60\n",
      "30/30 - 183s - loss: 0.3625 - accuracy: 0.8727 - val_loss: 0.4362 - val_accuracy: 0.8546 - 183s/epoch - 6s/step\n",
      "Epoch 48/60\n",
      "30/30 - 181s - loss: 0.3464 - accuracy: 0.8734 - val_loss: 0.4559 - val_accuracy: 0.8535 - 181s/epoch - 6s/step\n",
      "Epoch 49/60\n",
      "30/30 - 181s - loss: 0.3506 - accuracy: 0.8728 - val_loss: 0.4484 - val_accuracy: 0.8499 - 181s/epoch - 6s/step\n",
      "Epoch 50/60\n",
      "30/30 - 178s - loss: 0.3424 - accuracy: 0.8731 - val_loss: 0.4720 - val_accuracy: 0.8382 - 178s/epoch - 6s/step\n",
      "Epoch 51/60\n",
      "30/30 - 175s - loss: 0.3570 - accuracy: 0.8756 - val_loss: 0.4511 - val_accuracy: 0.8394 - 175s/epoch - 6s/step\n",
      "Epoch 52/60\n",
      "30/30 - 165s - loss: 0.3530 - accuracy: 0.8719 - val_loss: 0.4333 - val_accuracy: 0.8558 - 165s/epoch - 5s/step\n",
      "Epoch 53/60\n",
      "30/30 - 165s - loss: 0.3418 - accuracy: 0.8773 - val_loss: 0.4633 - val_accuracy: 0.8324 - 165s/epoch - 5s/step\n",
      "Epoch 54/60\n",
      "30/30 - 164s - loss: 0.3394 - accuracy: 0.8753 - val_loss: 0.4909 - val_accuracy: 0.8441 - 164s/epoch - 5s/step\n",
      "Epoch 55/60\n",
      "30/30 - 164s - loss: 0.3263 - accuracy: 0.8835 - val_loss: 0.4876 - val_accuracy: 0.8417 - 164s/epoch - 5s/step\n",
      "Epoch 56/60\n",
      "30/30 - 164s - loss: 0.3371 - accuracy: 0.8816 - val_loss: 0.4241 - val_accuracy: 0.8382 - 164s/epoch - 5s/step\n",
      "Epoch 57/60\n",
      "30/30 - 164s - loss: 0.3262 - accuracy: 0.8835 - val_loss: 0.4582 - val_accuracy: 0.8523 - 164s/epoch - 5s/step\n",
      "Epoch 58/60\n",
      "30/30 - 168s - loss: 0.3332 - accuracy: 0.8779 - val_loss: 0.4785 - val_accuracy: 0.8406 - 168s/epoch - 6s/step\n",
      "Epoch 59/60\n",
      "30/30 - 176s - loss: 0.3388 - accuracy: 0.8779 - val_loss: 0.4542 - val_accuracy: 0.8546 - 176s/epoch - 6s/step\n",
      "Epoch 60/60\n",
      "30/30 - 180s - loss: 0.3169 - accuracy: 0.8859 - val_loss: 0.4759 - val_accuracy: 0.8628 - 180s/epoch - 6s/step\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=256, epochs=60, verbose=2, shuffle=True)\n",
    "\n",
    "\n",
    "######saving weight\n",
    "model_json = model.to_json()\n",
    "with open(\"DL_ECG_classification_model\\model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"DL_ECG_classification_model\\model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "###Load model from json\n",
    "json_file = open('DL_ECG_classification_model\\model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"DL_ECG_classification_model\\model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 143ms/step\n",
      "Last epoch's validation score is  0.8628370457209847\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "predictions = model.predict(X_val)\n",
    "score = accuracy_score(change(Y_val), change(predictions))\n",
    "print('Last epoch\\'s validation score is ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT-BIH test size  75\n"
     ]
    }
   ],
   "source": [
    "#reading mitbih files:\n",
    "path = 'MyMIT-BIH'\n",
    "# print(allCsv)\n",
    "size=75\n",
    "print('MIT-BIH test size ', size)\n",
    "#fq=360Hz\n",
    "big = 12120\n",
    "MIT_BIH1 = np.zeros((size, big))\n",
    "\n",
    "for i in range(size):\n",
    "    dummy = pd.read_csv(path+\"/\"+str(i+1)+\".csv\", sep=',', header=None, names=None)[:].values.tolist()\n",
    "    dummy = np.reshape(dummy,(len(dummy)))\n",
    "    if(len(dummy)>=big):\n",
    "        MIT_BIH1[i,:] = dummy[:big]\n",
    "    elif(big-len(dummy)<=big/2):\n",
    "        b = dummy[0:(big - len(dummy))]\n",
    "        MIT_BIH1[i,:] = np.hstack((dummy, b))\n",
    "    else:\n",
    "        b=dummy\n",
    "        while len(dummy)!=big:\n",
    "            if(big-len(dummy)>=len(b)):\n",
    "                dummy = np.hstack((dummy,b))\n",
    "            else:\n",
    "                dummy= np.hstack((dummy,b[0:big - len(dummy)]))\n",
    "        MIT_BIH1[i, :] = dummy\n",
    "#normalize\n",
    "MIT_BIH1=(MIT_BIH1 - MIT_BIH1.mean()) / (MIT_BIH1.std())\n",
    "#downsampling\n",
    "MIT_BIH2=np.zeros((size, 10100))\n",
    "for i in range(size):\n",
    "    MIT_BIH2[i, :]=signal.resample(MIT_BIH1[i, :],10100)\n",
    "MIT_BIH2 = np.expand_dims(MIT_BIH2, axis=2)\n",
    "#Labels\n",
    "MyLabels=pd.read_csv(path+\"/Labels.csv\", sep=',', header=None, names=None)[:].values.tolist()\n",
    "MyLabels=np.reshape(MyLabels,(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 104ms/step\n",
      "[1 1 1 1 2 1 1 1 2 1 1 1 3 1 1 2 2 1 2 1 2 2 1 1 2 0 0 2 0 0 2 2 2 2 0 2 0\n",
      " 2 0 0 0 2 0 0 2 0 0 0 2 2 2 2 2 3 2 0 2 2 1 2 0 1 2 0 1 1 2 2 0 2 2 2 2 2\n",
      " 2]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2]\n",
      "The score is  0.6133333333333333\n",
      "16\n",
      "4\n",
      "25\n",
      "30\n",
      "precision is  0.8\n",
      "recall is  0.3902439024390244\n",
      "accuracy is  0.6133333333333333\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "prediction = model.predict(MIT_BIH2)\n",
    "print(change(prediction))\n",
    "print(MyLabels)\n",
    "score = accuracy_score(MyLabels, change(prediction))\n",
    "print('The score is ', score)\n",
    "TP=0\n",
    "FP=0\n",
    "FN=0\n",
    "TN=0\n",
    "for i in range(len(MyLabels)):\n",
    "    if change(prediction)[i]==1:\n",
    "        if MyLabels[i]==1:\n",
    "            TP=TP+1\n",
    "        else:\n",
    "            FP=FP+1\n",
    "    else:\n",
    "        if change(prediction)[i]==MyLabels[i]:\n",
    "            TN=TN+1\n",
    "        else:\n",
    "            FN=FN+1\n",
    "print(TP)\n",
    "print(FP)\n",
    "print(FN)\n",
    "print(TN)\n",
    "\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "accuracy=(TP+TN)/len(MyLabels)\n",
    "print(\"precision is \",precision)\n",
    "print(\"recall is \",recall)\n",
    "print(\"accuracy is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw graph\n",
    "\n",
    "# print(MIT_BIH1)\n",
    "# plt.figure(figsize=(20,15))\n",
    "# plt.plot(MIT_BIH1[74, :])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzfNU7HkAPav"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'MIT-BH'\n",
    "# onlyfiles1 = [f for f in listdir(path) if (isfile(join(path, f)) )]\n",
    "\n",
    "# # print(onlyfiles1)\n",
    "\n",
    "# allCsv= [f for f in onlyfiles1 if f[4] == 'c']\n",
    "\n",
    "\n",
    "# size=np.shape(allCsv)[0]\n",
    "# print('MIT-BH test size ', size)\n",
    "\n",
    "\n",
    "\n",
    "# big = 21600\n",
    "# MIT_BH = np.zeros((size, big))\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(size):\n",
    "#     dummy = pd.read_csv(path+\"/\"+allCsv[i], sep=',', header=None, names=None)[1][2:big+2].values.tolist()\n",
    "#     MIT_BH[i, :] = dummy\n",
    "\n",
    "# MIT_BH=(MIT_BH - MIT_BH.mean()) / (MIT_BH.std())\n",
    "# MIT_BH2=np.zeros((size, 18000))\n",
    "# MIT_BH3=np.zeros((size, 10100))\n",
    "# ##downsample\n",
    "\n",
    "# for i in range(size):\n",
    "#     MIT_BH2[i, :]=signal.resample(MIT_BH[i, :],18000)\n",
    "\n",
    "# ####\n",
    "# MIT_BH3=MIT_BH2[:,:10100]\n",
    "\n",
    "# MIT_BH3 = np.expand_dims(MIT_BH3, axis=2)\n",
    "\n",
    "\n",
    "# import warnings\n",
    "\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# prediction1 = model.predict(MIT_BH3)\n",
    "# print(change(prediction1))\n",
    "#label mit-bh\n",
    "# path2 = 'mitbih_database'\n",
    "# allFiles = [f for f in listdir(path2) if (isfile(join(path2, f)) )]\n",
    "# allTxt= [f for f in allFiles if f[3] == 'a']\n",
    "\n",
    "\n",
    "    \n",
    "# prediction_Num=change(prediction1)\n",
    "# ii=0\n",
    "# for filename in allTxt:\n",
    "#     f = open(path2+\"/\"+filename, \"r\")\n",
    "#     STR=f.read()\n",
    "#     if prediction_Num[ii]==0:\n",
    "#         if STR[93]==\" \":\n",
    "#             print('N   '+STR[90:92])\n",
    "#         else:\n",
    "#             print('N   '+STR[90:95])\n",
    "#     elif prediction_Num[ii]==1: \n",
    "#         if STR[93]==\" \":\n",
    "#             print('A   '+STR[90:92])\n",
    "#         else:\n",
    "#             print('A   '+STR[90:95])\n",
    "#     elif prediction_Num[ii]==2: \n",
    "#         if STR[93]==\" \":\n",
    "#             print('O   '+STR[90:92])\n",
    "#         else:\n",
    "#             print('O   '+STR[90:95])\n",
    "#     else:\n",
    "#         if STR[93]==\" \":\n",
    "#             print('~   '+STR[90:92])\n",
    "#         else:\n",
    "#             print('~   '+STR[90:95])\n",
    "#     ii=ii+1\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL ECG classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
